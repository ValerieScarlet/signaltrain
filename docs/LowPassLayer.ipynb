{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Tunable Low Pass Filter Layer\n",
    "This is just a note to self.  You should not have to read this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ''' Description:  This is a tunable 'reverse sigmoid' soft mask, which will get multiplied (element-wise)\n",
    "                      by the (vertical columns of) frequency spectrum, to serve as a low-pass filter.\n",
    "                      It has two parameters: the central frequency (from 0...1, scales with size of input)\n",
    "                      and a steepness parameter (also between -1.0=flat and 1.0=vertical)\n",
    "                      TODO: how do we best bound these to positive values?\n",
    "\n",
    "        Purpose:      Much of the loss-reduction in the final network output gets bound up in damping high-frequency noise.\n",
    "                      In theory we could let the network do this eventually, but this layer is to help converge faster.\n",
    "\n",
    "        Note:          Guide at https://discuss.pytorch.org/t/how-to-define-a-new-layer-with-autograd/351\n",
    "        \n",
    "        TODO:          Make indices work with batch_size\n",
    "    '''\n",
    "\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Background Tunable 'Tophat' Sigmoid Filter (in freq domain)\n",
    "\n",
    "$$ y(x) = \\left[ 1+ \\exp\\left( {1+m\\over 1+m+\\epsilon}10\\left({x\\over L} - b - 0.5\\right)  \\right) \\right]^{-1} $$\n",
    "where \n",
    "\n",
    "* $m$ is the tunable 'steepness' parameter  (-1 = horizontal, 0 = 'normal',  1 = vertical)\n",
    "* $b$ is the tunable 'center' parameter     (-0.5 = far left, 0 = middle, 0.5 = far right)\n",
    "* $L$ is the number of frequency bins \n",
    "* $x = \\{0..L\\}$ is the frequency bin\n",
    "* $\\epsilon=0.1$ is there just to keep things from blowing up when $m=1$. \n",
    "* '10' is in there just because it looked about right. ;-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3306bc4bae8246288584d3da11ade200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='m', max=1.0, min=-1.0, step=0.02), FloatSlider(value=0.0, description='b', max=1.0, min=-1.0, step=0.05), Output(layout=Layout(height='350px'))), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "L = 8192      # number of frequency bins in my fft output\n",
    "x = np.linspace(-1, L, num=100)     # num=100 is just for plotting. in code you want linspace(0,L-1,num=L)\n",
    "\n",
    "def y(m, b):\n",
    "    eps=0.1\n",
    "    plt.figure(2)\n",
    "    yvals = 1/(  1+np.exp( (1+m)/(1-m+eps)*10*(x/L-(b+0.5)) )  )\n",
    "    plt.plot(x, yvals) \n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "interactive_plot = interactive(y, m=(-1.0, 1.0,0.02), b=(-1, 1, 0.05))\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LowPassLayer(nn.Module):\n",
    "    ''' Description:  This is a tunable 'reverse sigmoid' soft mask, which will get multiplied (element-wise)\n",
    "                      by the (vertical columns of) frequency spectrum, to serve as a low-pass filter.\n",
    "                      It has two parameters: the central frequency (from 0...1, scales with size of input)\n",
    "                      and a steepness parameter (also between -1.0=flat and 1.0=vertical)\n",
    "                      TODO: how do we best bound these to positive values?\n",
    "\n",
    "        Purpose:      Much of the loss-reduction in the final network output gets bound up in damping high-frequency noise.\n",
    "                      In theory we could let the network do this eventually, but this layer is to help converge faster.\n",
    "\n",
    "        Note:          Guide at https://discuss.pytorch.org/t/how-to-define-a-new-layer-with-autograd/351\n",
    "        \n",
    "        TODO:          Make indices work with batch_size\n",
    "    '''\n",
    "    def __init__(self, bins):\n",
    "        super(LowPassLayer, self).__init__()\n",
    "        self.m = nn.Parameter(torch.zeros(1))      # steepness, tunable parameter, default is 0\n",
    "        self.b = nn.Parameter(torch.zeros(1))      # center, tunable parameter, default is 0\n",
    "        self.eps = 0.1\n",
    "        self.bins = bins\n",
    "        self.x = Variable(torch.linspace(0, bins-1, num=bins))\n",
    "        \n",
    "    def forward(self, in_stft):\n",
    "        # given matrix A (n x m) and vector v (n elements), the operation we want is (A.T * v).T\n",
    "        #   This method is also the fastest: https://stackoverflow.com/questions/18522216/multiplying-across-in-a-numpy-array\n",
    "\n",
    "        m = self.m.expand_as(self.x)     # make it a 1-D vector of repeated numbers\n",
    "        b = self.b.expand_as(self.x)\n",
    "        mask = 1/(1+torch.exp( (1+m)/(1-m+self.eps)*10*(self.x/self.bins-(b+0.5)) )  ) \n",
    "        out_stft = torch.transpose( torch.mm( torch.transpose(in_stft), mask ) )\n",
    "        return out_stft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
